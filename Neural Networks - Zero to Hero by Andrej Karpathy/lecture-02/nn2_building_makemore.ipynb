{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 29681 unique names and saved to cleaned_names.txt.\n"
     ]
    }
   ],
   "source": [
    "# Input file (already renamed)\n",
    "input_file = \"names.txt\"\n",
    "\n",
    "# Output file to save cleaned names\n",
    "output_file = \"cleaned_names.txt\"\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    names = set()  # Use a set to remove duplicates\n",
    "    for line in infile:\n",
    "        # Split by commas and extract the name (first column)\n",
    "        name = line.split(\",\")[0]\n",
    "        # Convert to lowercase and strip extra spaces\n",
    "        names.add(name.strip().lower())\n",
    "\n",
    "    # Write the cleaned names to the new file (no numbering)\n",
    "    for name in sorted(names):  # Optional: Sort names alphabetically\n",
    "        outfile.write(f\"{name}\\n\")  # Write each name on a new line\n",
    "\n",
    "print(f\"Cleaned {len(names)} unique names and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('cleaned_names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaban',\n",
       " 'aabid',\n",
       " 'aabidah',\n",
       " 'aabir',\n",
       " 'aabriella',\n",
       " 'aada',\n",
       " 'aadam',\n",
       " 'aadarsh',\n",
       " 'aadaya',\n",
       " 'aaden']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29681"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['</E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1\n",
    "    # print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<S>', 'a'): 4155,\n",
       " ('a', 'a'): 547,\n",
       " ('a', 'b'): 531,\n",
       " ('b', 'a'): 302,\n",
       " ('a', 'n'): 5120,\n",
       " ('n', '</E>'): 6040,\n",
       " ('b', 'i'): 198,\n",
       " ('i', 'd'): 408,\n",
       " ('d', '</E>'): 488,\n",
       " ('d', 'a'): 1226,\n",
       " ('a', 'h'): 2258,\n",
       " ('h', '</E>'): 2318,\n",
       " ('i', 'r'): 816,\n",
       " ('r', '</E>'): 1232,\n",
       " ('b', 'r'): 750,\n",
       " ('r', 'i'): 2785,\n",
       " ('i', 'e'): 1546,\n",
       " ('e', 'l'): 3093,\n",
       " ('l', 'l'): 1273,\n",
       " ('l', 'a'): 2482,\n",
       " ('a', '</E>'): 6508,\n",
       " ('a', 'd'): 977,\n",
       " ('a', 'm'): 1512,\n",
       " ('m', '</E>'): 479,\n",
       " ('a', 'r'): 3030,\n",
       " ('r', 's'): 167,\n",
       " ('s', 'h'): 1230,\n",
       " ('a', 'y'): 1904,\n",
       " ('y', 'a'): 2088,\n",
       " ('d', 'e'): 1169,\n",
       " ('e', 'n'): 2417,\n",
       " ('d', 'h'): 116,\n",
       " ('h', 'a'): 2112,\n",
       " ('a', 'v'): 789,\n",
       " ('v', '</E>'): 86,\n",
       " ('v', 'a'): 614,\n",
       " ('h', 'i'): 704,\n",
       " ('i', '</E>'): 2233,\n",
       " ('r', 'a'): 2247,\n",
       " ('h', 'v'): 39,\n",
       " ('v', 'i'): 880,\n",
       " ('i', 'k'): 425,\n",
       " ('k', '</E>'): 356,\n",
       " ('k', 'a'): 1607,\n",
       " ('h', 'y'): 191,\n",
       " ('r', 'e'): 1553,\n",
       " ('e', 'd'): 363,\n",
       " ('d', 'd'): 138,\n",
       " ('d', 'y'): 268,\n",
       " ('y', '</E>'): 1737,\n",
       " ('a', 's'): 1053,\n",
       " ('s', 'r'): 55,\n",
       " ('d', 'i'): 622,\n",
       " ('i', 'l'): 1282,\n",
       " ('l', '</E>'): 1223,\n",
       " ('i', 's'): 1218,\n",
       " ('i', 't'): 518,\n",
       " ('t', '</E>'): 439,\n",
       " ('t', 'h'): 626,\n",
       " ('t', 'r'): 336,\n",
       " ('t', 'y'): 312,\n",
       " ('d', 'v'): 17,\n",
       " ('n', 't'): 421,\n",
       " ('y', 'n'): 1641,\n",
       " ('a', 'f'): 132,\n",
       " ('f', 'i'): 147,\n",
       " ('i', 'y'): 763,\n",
       " ('a', 'g'): 155,\n",
       " ('g', 'a'): 292,\n",
       " ('n', 'a'): 2923,\n",
       " ('h', 'e'): 631,\n",
       " ('l', 'i'): 2361,\n",
       " ('a', 'i'): 1552,\n",
       " ('l', 'y'): 1487,\n",
       " ('i', 'm'): 405,\n",
       " ('m', 'a'): 2451,\n",
       " ('a', 'k'): 528,\n",
       " ('k', 'i'): 472,\n",
       " ('i', 'f'): 94,\n",
       " ('f', '</E>'): 78,\n",
       " ('k', 'r'): 100,\n",
       " ('t', 'i'): 495,\n",
       " ('a', 'l'): 2431,\n",
       " ('n', 'i'): 1607,\n",
       " ('s', 'i'): 646,\n",
       " ('i', 'a'): 2347,\n",
       " ('l', 'e'): 2688,\n",
       " ('e', 'a'): 657,\n",
       " ('e', 'e'): 1205,\n",
       " ('e', 'y'): 942,\n",
       " ('e', 'i'): 793,\n",
       " ('i', 'g'): 399,\n",
       " ('g', 'h'): 342,\n",
       " ('i', 'j'): 74,\n",
       " ('j', 'a'): 1381,\n",
       " ('i', 'n'): 1963,\n",
       " ('h', 'm'): 111,\n",
       " ('e', '</E>'): 3676,\n",
       " ('h', 'r'): 197,\n",
       " ('r', 'o'): 800,\n",
       " ('o', 's'): 486,\n",
       " ('s', 'e'): 832,\n",
       " ('y', 'h'): 22,\n",
       " ('y', 'i'): 189,\n",
       " ('m', 'e'): 771,\n",
       " ('e', 'r'): 1772,\n",
       " ('m', 'i'): 1177,\n",
       " ('m', 'n'): 17,\n",
       " ('m', 'o'): 418,\n",
       " ('o', 'r'): 952,\n",
       " ('n', 'd'): 642,\n",
       " ('n', 's'): 265,\n",
       " ('n', 'v'): 54,\n",
       " ('n', 'y'): 440,\n",
       " ('a', 'q'): 58,\n",
       " ('q', 'i'): 12,\n",
       " ('b', 'h'): 41,\n",
       " ('n', 'n'): 1818,\n",
       " ('i', 'b'): 110,\n",
       " ('b', '</E>'): 110,\n",
       " ('i', 'c'): 482,\n",
       " ('c', '</E>'): 94,\n",
       " ('i', 'o'): 565,\n",
       " ('o', 'n'): 2213,\n",
       " ('s', '</E>'): 1082,\n",
       " ('i', 'v'): 251,\n",
       " ('i', 'z'): 269,\n",
       " ('z', '</E>'): 154,\n",
       " ('r', 'n'): 131,\n",
       " ('o', '</E>'): 817,\n",
       " ('o', 'h'): 160,\n",
       " ('r', 'r'): 392,\n",
       " ('r', 'u'): 235,\n",
       " ('u', 'h'): 58,\n",
       " ('u', 'n'): 256,\n",
       " ('u', 's'): 458,\n",
       " ('r', 'v'): 75,\n",
       " ('r', 'y'): 681,\n",
       " ('r', 'z'): 22,\n",
       " ('z', 'a'): 821,\n",
       " ('h', 'k'): 31,\n",
       " ('h', 'n'): 135,\n",
       " ('s', 'o'): 472,\n",
       " ('s', 't'): 685,\n",
       " ('v', 'y'): 113,\n",
       " ('a', 't'): 656,\n",
       " ('y', 'd'): 244,\n",
       " ('y', 'l'): 985,\n",
       " ('y', 'r'): 278,\n",
       " ('y', 'u'): 136,\n",
       " ('a', 'z'): 402,\n",
       " ('z', 'e'): 357,\n",
       " ('n', 'o'): 455,\n",
       " ('o', 'u'): 251,\n",
       " ('u', 'b'): 104,\n",
       " ('s', 's'): 438,\n",
       " ('b', 'b'): 35,\n",
       " ('b', 'e'): 631,\n",
       " ('b', 'o'): 92,\n",
       " ('o', 't'): 105,\n",
       " ('t', 't'): 340,\n",
       " ('b', 'y'): 64,\n",
       " ('y', 'g'): 27,\n",
       " ('b', 'c'): 1,\n",
       " ('c', 'd'): 1,\n",
       " ('b', 'd'): 65,\n",
       " ('l', 'r'): 18,\n",
       " ('z', 'i'): 337,\n",
       " ('f', 'a'): 237,\n",
       " ('t', 'a'): 978,\n",
       " ('i', 'h'): 94,\n",
       " ('q', '</E>'): 28,\n",
       " ('k', 'u'): 48,\n",
       " ('u', 'r'): 374,\n",
       " ('d', 'o'): 361,\n",
       " ('u', '</E>'): 143,\n",
       " ('u', 'l'): 293,\n",
       " ('y', 'e'): 287,\n",
       " ('n', 'e'): 1284,\n",
       " ('d', 'r'): 407,\n",
       " ('d', 'u'): 90,\n",
       " ('e', 'z'): 170,\n",
       " ('l', 'b'): 46,\n",
       " ('l', 'g'): 6,\n",
       " ('l', 'h'): 18,\n",
       " ('l', 'j'): 6,\n",
       " ('l', 'k'): 23,\n",
       " ('e', 'm'): 716,\n",
       " ('l', 'o'): 622,\n",
       " ('l', 'm'): 59,\n",
       " ('h', 's'): 29,\n",
       " ('l', 'q'): 3,\n",
       " ('q', 'a'): 12,\n",
       " ('l', 's'): 87,\n",
       " ('s', 'a'): 1158,\n",
       " ('l', 'w'): 16,\n",
       " ('w', 'a'): 243,\n",
       " ('e', 'h'): 143,\n",
       " ('d', 'n'): 27,\n",
       " ('e', 'g'): 104,\n",
       " ('g', 'o'): 77,\n",
       " ('r', 'd'): 174,\n",
       " ('g', 'n'): 25,\n",
       " ('a', 'j'): 169,\n",
       " ('j', '</E>'): 66,\n",
       " ('e', 'k'): 174,\n",
       " ('a', 'e'): 651,\n",
       " ('g', 'i'): 179,\n",
       " ('e', 'c'): 138,\n",
       " ('o', 'l'): 554,\n",
       " ('b', 'l'): 83,\n",
       " ('b', 'n'): 4,\n",
       " ('a', 'c'): 430,\n",
       " ('c', 'a'): 738,\n",
       " ('a', 'x'): 165,\n",
       " ('x', 'a'): 100,\n",
       " ('r', 'h'): 105,\n",
       " ('u', 'm'): 146,\n",
       " ('b', 's'): 8,\n",
       " ('o', 'm'): 240,\n",
       " ('b', 'u'): 44,\n",
       " ('k', 'e'): 816,\n",
       " ('u', 'k'): 87,\n",
       " ('c', 'i'): 268,\n",
       " ('c', 'c'): 38,\n",
       " ('c', 'e'): 498,\n",
       " ('e', 'o'): 255,\n",
       " ('e', 's'): 793,\n",
       " ('t', 'o'): 589,\n",
       " ('s', 'y'): 191,\n",
       " ('e', 't'): 561,\n",
       " ('c', 'h'): 607,\n",
       " ('e', 'u'): 68,\n",
       " ('h', 'o'): 265,\n",
       " ('u', 't'): 74,\n",
       " ('c', 's'): 5,\n",
       " ('c', 't'): 33,\n",
       " ('c', 'x'): 3,\n",
       " ('x', 'e'): 34,\n",
       " ('h', 'l'): 174,\n",
       " ('r', 't'): 192,\n",
       " ('y', 'z'): 79,\n",
       " ('y', 's'): 383,\n",
       " ('m', 's'): 30,\n",
       " ('a', 'o'): 62,\n",
       " ('i', 'u'): 111,\n",
       " ('c', 'u'): 34,\n",
       " ('d', 'l'): 50,\n",
       " ('e', 'b'): 116,\n",
       " ('o', 'w'): 90,\n",
       " ('y', 'o'): 256,\n",
       " ('e', 'v'): 430,\n",
       " ('l', 'u'): 288,\n",
       " ('u', 'w'): 64,\n",
       " ('e', 'w'): 43,\n",
       " ('s', 'u'): 174,\n",
       " ('o', 'k'): 61,\n",
       " ('n', 'b'): 8,\n",
       " ('d', 'j'): 9,\n",
       " ('d', 'm'): 29,\n",
       " ('l', 'f'): 22,\n",
       " ('f', 'o'): 54,\n",
       " ('l', 'p'): 14,\n",
       " ('p', 'h'): 188,\n",
       " ('h', 'u'): 155,\n",
       " ('c', 'k'): 289,\n",
       " ('d', 'w'): 22,\n",
       " ('w', 'i'): 133,\n",
       " ('w', 'o'): 36,\n",
       " ('o', 'a'): 132,\n",
       " ('n', 'g'): 249,\n",
       " ('g', 'u'): 78,\n",
       " ('o', 'p'): 87,\n",
       " ('p', '</E>'): 33,\n",
       " ('f', 'e'): 115,\n",
       " ('f', 'f'): 41,\n",
       " ('f', 'n'): 4,\n",
       " ('f', 'r'): 108,\n",
       " ('f', 's'): 6,\n",
       " ('f', 't'): 16,\n",
       " ('m', 'j'): 6,\n",
       " ('j', 'o'): 443,\n",
       " ('m', 'v'): 3,\n",
       " ('v', 'e'): 516,\n",
       " ('a', 'p'): 77,\n",
       " ('p', 'e'): 168,\n",
       " ('n', 'u'): 91,\n",
       " ('g', '</E>'): 101,\n",
       " ('c', 'l'): 106,\n",
       " ('o', 'd'): 166,\n",
       " ('m', 'y'): 274,\n",
       " ('h', 't'): 56,\n",
       " ('t', 'z'): 104,\n",
       " ('u', 'v'): 36,\n",
       " ('h', 'z'): 20,\n",
       " ('n', 'j'): 44,\n",
       " ('i', 'i'): 80,\n",
       " ('k', 'o'): 297,\n",
       " ('n', 'h'): 24,\n",
       " ('s', 'l'): 255,\n",
       " ('n', 'z'): 140,\n",
       " ('z', 'l'): 123,\n",
       " ('r', 'l'): 365,\n",
       " ('h', 'w'): 10,\n",
       " ('i', 'x'): 71,\n",
       " ('x', '</E>'): 121,\n",
       " ('j', 'd'): 4,\n",
       " ('j', 'e'): 412,\n",
       " ('j', 'i'): 109,\n",
       " ('j', 'l'): 9,\n",
       " ('j', 'n'): 3,\n",
       " ('o', 'o'): 101,\n",
       " ('j', 's'): 7,\n",
       " ('j', 'u'): 183,\n",
       " ('j', 'w'): 6,\n",
       " ('k', 'b'): 2,\n",
       " ('k', 'h'): 285,\n",
       " ('o', 'i'): 66,\n",
       " ('u', 'a'): 162,\n",
       " ('k', 's'): 92,\n",
       " ('k', 'y'): 337,\n",
       " ('u', 'd'): 126,\n",
       " ('t', 'e'): 665,\n",
       " ('z', 'o'): 105,\n",
       " ('s', 'd'): 8,\n",
       " ('s', 'k'): 76,\n",
       " ('a', 'u'): 359,\n",
       " ('y', 'j'): 22,\n",
       " ('z', 'n'): 4,\n",
       " ('l', 'c'): 26,\n",
       " ('l', 'd'): 130,\n",
       " ('k', 'z'): 2,\n",
       " ('c', 'z'): 4,\n",
       " ('e', 'f'): 80,\n",
       " ('g', 'e'): 311,\n",
       " ('g', 'r'): 183,\n",
       " ('e', 'j'): 56,\n",
       " ('e', 'p'): 77,\n",
       " ('e', 'x'): 120,\n",
       " ('r', 'j'): 25,\n",
       " ('x', 'c'): 4,\n",
       " ('x', 'i'): 97,\n",
       " ('x', 's'): 29,\n",
       " ('x', 'u'): 5,\n",
       " ('x', 'x'): 34,\n",
       " ('x', 'y'): 28,\n",
       " ('x', 'z'): 19,\n",
       " ('c', 'j'): 3,\n",
       " ('c', 'y'): 86,\n",
       " ('h', 'b'): 6,\n",
       " ('z', 'z'): 41,\n",
       " ('u', 'e'): 156,\n",
       " ('o', 'y'): 96,\n",
       " ('l', 't'): 73,\n",
       " ('u', 'c'): 97,\n",
       " ('l', 'v'): 69,\n",
       " ('w', 'y'): 63,\n",
       " ('y', 'c'): 103,\n",
       " ('y', 'v'): 102,\n",
       " ('y', 'x'): 18,\n",
       " ('h', 'd'): 23,\n",
       " ('k', 'w'): 32,\n",
       " ('w', 'u'): 23,\n",
       " ('t', 'u'): 68,\n",
       " ('m', 'b'): 107,\n",
       " ('c', 'o'): 334,\n",
       " ('r', 'k'): 81,\n",
       " ('o', 'j'): 16,\n",
       " ('m', 'm'): 153,\n",
       " ('o', 'g'): 37,\n",
       " ('m', 'r'): 85,\n",
       " ('m', 'u'): 136,\n",
       " ('n', 'r'): 40,\n",
       " ('y', 't'): 92,\n",
       " ('m', 'z'): 11,\n",
       " ('u', 'i'): 113,\n",
       " ('m', 't'): 4,\n",
       " ('p', 'a'): 195,\n",
       " ('o', 'f'): 33,\n",
       " ('z', 'j'): 2,\n",
       " ('n', 'c'): 184,\n",
       " ('w', '</E>'): 39,\n",
       " ('w', 's'): 16,\n",
       " ('n', 'f'): 13,\n",
       " ('i', 'q'): 49,\n",
       " ('q', 'u'): 195,\n",
       " ('g', 'y'): 28,\n",
       " ('k', 'k'): 21,\n",
       " ('n', 'k'): 51,\n",
       " ('n', 'm'): 17,\n",
       " ('n', 'l'): 173,\n",
       " ('o', 'v'): 167,\n",
       " ('n', 'q'): 2,\n",
       " ('t', 'j'): 3,\n",
       " ('t', 'w'): 11,\n",
       " ('u', 'j'): 14,\n",
       " ('u', 'o'): 7,\n",
       " ('p', 'o'): 58,\n",
       " ('n', 'w'): 12,\n",
       " ('w', 'e'): 128,\n",
       " ('y', 'f'): 11,\n",
       " ('p', 'p'): 35,\n",
       " ('p', 'l'): 13,\n",
       " ('p', 'r'): 139,\n",
       " ('p', 's'): 14,\n",
       " ('q', 's'): 2,\n",
       " ('t', 'x'): 2,\n",
       " ('r', 'b'): 33,\n",
       " ('r', 'c'): 91,\n",
       " ('z', 'y'): 140,\n",
       " ('r', 'f'): 9,\n",
       " ('r', 'g'): 74,\n",
       " ('s', 'b'): 19,\n",
       " ('t', 'l'): 112,\n",
       " ('z', 'b'): 4,\n",
       " ('g', 't'): 22,\n",
       " ('r', 'm'): 147,\n",
       " ('r', 'p'): 12,\n",
       " ('p', 'i'): 55,\n",
       " ('r', 'w'): 21,\n",
       " ('y', 'b'): 28,\n",
       " ('z', 'u'): 69,\n",
       " ('d', 'b'): 1,\n",
       " ('s', 'c'): 53,\n",
       " ('a', 'w'): 151,\n",
       " ('w', 'n'): 54,\n",
       " ('h', 'f'): 2,\n",
       " ('s', 'j'): 2,\n",
       " ('s', 'm'): 91,\n",
       " ('s', 'p'): 46,\n",
       " ('p', 'y'): 10,\n",
       " ('u', 'g'): 43,\n",
       " ('g', 'g'): 21,\n",
       " ('y', 'm'): 141,\n",
       " ('y', 'k'): 78,\n",
       " ('g', 'd'): 19,\n",
       " ('v', 'l'): 13,\n",
       " ('v', 'n'): 8,\n",
       " ('v', 'o'): 146,\n",
       " ('v', 'r'): 44,\n",
       " ('k', 't'): 17,\n",
       " ('x', 'l'): 32,\n",
       " ('x', 'o'): 36,\n",
       " ('x', 't'): 63,\n",
       " ('o', 'b'): 120,\n",
       " ('i', 'p'): 49,\n",
       " ('u', 'u'): 3,\n",
       " ('y', 'y'): 23,\n",
       " ('z', 'h'): 38,\n",
       " ('z', 'k'): 2,\n",
       " ('z', 'm'): 35,\n",
       " ('z', 'r'): 25,\n",
       " ('z', 't'): 4,\n",
       " ('<S>', 'b'): 1153,\n",
       " ('h', 'j'): 9,\n",
       " ('n', 'x'): 4,\n",
       " ('p', 't'): 15,\n",
       " ('t', 's'): 34,\n",
       " ('f', 'u'): 11,\n",
       " ('u', 'x'): 27,\n",
       " ('k', 'l'): 123,\n",
       " ('k', 'm'): 9,\n",
       " ('d', 'f'): 5,\n",
       " ('u', 'z'): 43,\n",
       " ('x', 'b'): 1,\n",
       " ('b', 'j'): 2,\n",
       " ('g', 's'): 23,\n",
       " ('o', 'c'): 107,\n",
       " ('t', 'v'): 15,\n",
       " ('o', 'e'): 115,\n",
       " ('w', 'd'): 7,\n",
       " ('w', 'm'): 2,\n",
       " ('d', 't'): 4,\n",
       " ('w', 'l'): 12,\n",
       " ('x', 'd'): 5,\n",
       " ('o', 'x'): 37,\n",
       " ('d', 'g'): 23,\n",
       " ('t', 'n'): 19,\n",
       " ('<S>', 'c'): 1398,\n",
       " ('y', 'p'): 14,\n",
       " ('m', 'd'): 20,\n",
       " ('m', 'p'): 32,\n",
       " ('p', 'b'): 1,\n",
       " ('m', 'h'): 5,\n",
       " ('s', 'w'): 21,\n",
       " ('p', 'm'): 1,\n",
       " ('s', 'n'): 22,\n",
       " ('o', 'z'): 54,\n",
       " ('t', 'm'): 4,\n",
       " ('t', 'c'): 15,\n",
       " ('m', 'f'): 1,\n",
       " ('p', 'c'): 1,\n",
       " ('v', 'u'): 8,\n",
       " ('c', 'r'): 69,\n",
       " ('w', 'f'): 2,\n",
       " ('u', 'y'): 12,\n",
       " ('<S>', 'd'): 1579,\n",
       " ('e', 'q'): 13,\n",
       " ('g', 'm'): 6,\n",
       " ('i', 'w'): 8,\n",
       " ('d', 's'): 24,\n",
       " ('w', 't'): 8,\n",
       " ('y', 'q'): 6,\n",
       " ('g', 'l'): 29,\n",
       " ('r', 'q'): 16,\n",
       " ('l', 'z'): 7,\n",
       " ('l', 'n'): 12,\n",
       " ('d', 'k'): 3,\n",
       " ('v', 'b'): 1,\n",
       " ('u', 'p'): 13,\n",
       " ('<S>', 'e'): 1435,\n",
       " ('f', 'y'): 12,\n",
       " ('k', 'n'): 21,\n",
       " ('j', 'h'): 45,\n",
       " ('h', 'g'): 4,\n",
       " ('m', 'l'): 6,\n",
       " ('s', 'g'): 2,\n",
       " ('p', 'n'): 1,\n",
       " ('s', 'v'): 14,\n",
       " ('u', 'f'): 19,\n",
       " ('v', 'v'): 7,\n",
       " ('<S>', 'f'): 383,\n",
       " ('j', 'r'): 8,\n",
       " ('o', 'q'): 3,\n",
       " ('u', 'q'): 10,\n",
       " ('w', 'k'): 6,\n",
       " ('w', 'w'): 2,\n",
       " ('w', 'z'): 1,\n",
       " ('z', 'g'): 1,\n",
       " ('z', 'p'): 2,\n",
       " ('z', 'w'): 3,\n",
       " ('f', 'l'): 18,\n",
       " ('s', 'z'): 10,\n",
       " ('j', 'y'): 10,\n",
       " ('<S>', 'g'): 620,\n",
       " ('w', 'r'): 18,\n",
       " ('g', 'w'): 26,\n",
       " ('<S>', 'h'): 798,\n",
       " ('j', 'j'): 2,\n",
       " ('t', 'f'): 2,\n",
       " ('y', 'w'): 4,\n",
       " ('f', 'z'): 2,\n",
       " ('v', 'h'): 1,\n",
       " ('<S>', 'i'): 553,\n",
       " ('b', 't'): 2,\n",
       " ('t', 'b'): 2,\n",
       " ('q', 'l'): 1,\n",
       " ('q', 'r'): 1,\n",
       " ('z', 's'): 4,\n",
       " ('<S>', 'j'): 2239,\n",
       " ('c', 'q'): 11,\n",
       " ('d', 'z'): 1,\n",
       " ('h', 'c'): 2,\n",
       " ('h', 'q'): 1,\n",
       " ('p', 'j'): 1,\n",
       " ('x', 'n'): 1,\n",
       " ('j', 'b'): 1,\n",
       " ('j', 'c'): 4,\n",
       " ('n', 'p'): 5,\n",
       " ('j', 'k'): 2,\n",
       " ('j', 'm'): 5,\n",
       " ('j', 'p'): 2,\n",
       " ('j', 't'): 2,\n",
       " ('j', 'v'): 5,\n",
       " ('<S>', 'k'): 2700,\n",
       " ('c', 'p'): 1,\n",
       " ('w', 'h'): 20,\n",
       " ('z', 'd'): 2,\n",
       " ('k', 'c'): 1,\n",
       " ('k', 'd'): 2,\n",
       " ('g', 'j'): 3,\n",
       " ('g', 'z'): 1,\n",
       " ('k', 'j'): 2,\n",
       " ('k', 'v'): 2,\n",
       " ('<S>', 'l'): 1451,\n",
       " ('q', 'm'): 2,\n",
       " ('<S>', 'm'): 2401,\n",
       " ('c', 'g'): 2,\n",
       " ('h', 'p'): 1,\n",
       " ('r', 'x'): 3,\n",
       " ('x', 'f'): 3,\n",
       " ('x', 'm'): 1,\n",
       " ('x', 'w'): 2,\n",
       " ('m', 'c'): 44,\n",
       " ('t', 'g'): 1,\n",
       " ('d', 'c'): 3,\n",
       " ('w', 'g'): 1,\n",
       " ('s', 'f'): 2,\n",
       " ('<S>', 'n'): 1090,\n",
       " ('v', 'd'): 1,\n",
       " ('h', 'h'): 1,\n",
       " ('<S>', 'o'): 358,\n",
       " ('m', 'k'): 1,\n",
       " ('<S>', 'p'): 467,\n",
       " ('s', 'q'): 1,\n",
       " ('p', 'f'): 1,\n",
       " ('p', 'u'): 4,\n",
       " ('<S>', 'q'): 85,\n",
       " ('q', 'w'): 3,\n",
       " ('<S>', 'r'): 1471,\n",
       " ('f', 'k'): 2,\n",
       " ('g', 'b'): 2,\n",
       " ('p', 'k'): 1,\n",
       " ('v', 'k'): 3,\n",
       " ('k', 'f'): 1,\n",
       " ('w', 'b'): 1,\n",
       " ('<S>', 's'): 1914,\n",
       " ('f', 'w'): 4,\n",
       " ('m', 'w'): 2,\n",
       " ('g', 'f'): 1,\n",
       " ('f', 'h'): 1,\n",
       " ('<S>', 't'): 1209,\n",
       " ('g', 'v'): 1,\n",
       " ('z', 'v'): 2,\n",
       " ('<S>', 'u'): 71,\n",
       " ('<S>', 'v'): 363,\n",
       " ('<S>', 'w'): 262,\n",
       " ('f', 'g'): 1,\n",
       " ('<S>', 'x'): 131,\n",
       " ('x', 'h'): 1,\n",
       " ('<S>', 'y'): 519,\n",
       " ('q', 'e'): 1,\n",
       " ('q', 'o'): 2,\n",
       " ('z', 'c'): 2,\n",
       " ('<S>', 'z'): 876,\n",
       " ('d', 'q'): 1,\n",
       " ('z', 'x'): 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of all the words\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '</E>'), 6508),\n",
       " (('n', '</E>'), 6040),\n",
       " (('a', 'n'), 5120),\n",
       " (('<S>', 'a'), 4155),\n",
       " (('e', '</E>'), 3676),\n",
       " (('e', 'l'), 3093),\n",
       " (('a', 'r'), 3030),\n",
       " (('n', 'a'), 2923),\n",
       " (('r', 'i'), 2785),\n",
       " (('<S>', 'k'), 2700),\n",
       " (('l', 'e'), 2688),\n",
       " (('l', 'a'), 2482),\n",
       " (('m', 'a'), 2451),\n",
       " (('a', 'l'), 2431),\n",
       " (('e', 'n'), 2417),\n",
       " (('<S>', 'm'), 2401),\n",
       " (('l', 'i'), 2361),\n",
       " (('i', 'a'), 2347),\n",
       " (('h', '</E>'), 2318),\n",
       " (('a', 'h'), 2258),\n",
       " (('r', 'a'), 2247),\n",
       " (('<S>', 'j'), 2239),\n",
       " (('i', '</E>'), 2233),\n",
       " (('o', 'n'), 2213),\n",
       " (('h', 'a'), 2112),\n",
       " (('y', 'a'), 2088),\n",
       " (('i', 'n'), 1963),\n",
       " (('<S>', 's'), 1914),\n",
       " (('a', 'y'), 1904),\n",
       " (('n', 'n'), 1818),\n",
       " (('e', 'r'), 1772),\n",
       " (('y', '</E>'), 1737),\n",
       " (('y', 'n'), 1641),\n",
       " (('k', 'a'), 1607),\n",
       " (('n', 'i'), 1607),\n",
       " (('<S>', 'd'), 1579),\n",
       " (('r', 'e'), 1553),\n",
       " (('a', 'i'), 1552),\n",
       " (('i', 'e'), 1546),\n",
       " (('a', 'm'), 1512),\n",
       " (('l', 'y'), 1487),\n",
       " (('<S>', 'r'), 1471),\n",
       " (('<S>', 'l'), 1451),\n",
       " (('<S>', 'e'), 1435),\n",
       " (('<S>', 'c'), 1398),\n",
       " (('j', 'a'), 1381),\n",
       " (('n', 'e'), 1284),\n",
       " (('i', 'l'), 1282),\n",
       " (('l', 'l'), 1273),\n",
       " (('r', '</E>'), 1232),\n",
       " (('s', 'h'), 1230),\n",
       " (('d', 'a'), 1226),\n",
       " (('l', '</E>'), 1223),\n",
       " (('i', 's'), 1218),\n",
       " (('<S>', 't'), 1209),\n",
       " (('e', 'e'), 1205),\n",
       " (('m', 'i'), 1177),\n",
       " (('d', 'e'), 1169),\n",
       " (('s', 'a'), 1158),\n",
       " (('<S>', 'b'), 1153),\n",
       " (('<S>', 'n'), 1090),\n",
       " (('s', '</E>'), 1082),\n",
       " (('a', 's'), 1053),\n",
       " (('y', 'l'), 985),\n",
       " (('t', 'a'), 978),\n",
       " (('a', 'd'), 977),\n",
       " (('o', 'r'), 952),\n",
       " (('e', 'y'), 942),\n",
       " (('v', 'i'), 880),\n",
       " (('<S>', 'z'), 876),\n",
       " (('s', 'e'), 832),\n",
       " (('z', 'a'), 821),\n",
       " (('o', '</E>'), 817),\n",
       " (('i', 'r'), 816),\n",
       " (('k', 'e'), 816),\n",
       " (('r', 'o'), 800),\n",
       " (('<S>', 'h'), 798),\n",
       " (('e', 'i'), 793),\n",
       " (('e', 's'), 793),\n",
       " (('a', 'v'), 789),\n",
       " (('m', 'e'), 771),\n",
       " (('i', 'y'), 763),\n",
       " (('b', 'r'), 750),\n",
       " (('c', 'a'), 738),\n",
       " (('e', 'm'), 716),\n",
       " (('h', 'i'), 704),\n",
       " (('s', 't'), 685),\n",
       " (('r', 'y'), 681),\n",
       " (('t', 'e'), 665),\n",
       " (('e', 'a'), 657),\n",
       " (('a', 't'), 656),\n",
       " (('a', 'e'), 651),\n",
       " (('s', 'i'), 646),\n",
       " (('n', 'd'), 642),\n",
       " (('h', 'e'), 631),\n",
       " (('b', 'e'), 631),\n",
       " (('t', 'h'), 626),\n",
       " (('d', 'i'), 622),\n",
       " (('l', 'o'), 622),\n",
       " (('<S>', 'g'), 620),\n",
       " (('v', 'a'), 614),\n",
       " (('c', 'h'), 607),\n",
       " (('t', 'o'), 589),\n",
       " (('i', 'o'), 565),\n",
       " (('e', 't'), 561),\n",
       " (('o', 'l'), 554),\n",
       " (('<S>', 'i'), 553),\n",
       " (('a', 'a'), 547),\n",
       " (('a', 'b'), 531),\n",
       " (('a', 'k'), 528),\n",
       " (('<S>', 'y'), 519),\n",
       " (('i', 't'), 518),\n",
       " (('v', 'e'), 516),\n",
       " (('c', 'e'), 498),\n",
       " (('t', 'i'), 495),\n",
       " (('d', '</E>'), 488),\n",
       " (('o', 's'), 486),\n",
       " (('i', 'c'), 482),\n",
       " (('m', '</E>'), 479),\n",
       " (('k', 'i'), 472),\n",
       " (('s', 'o'), 472),\n",
       " (('<S>', 'p'), 467),\n",
       " (('u', 's'), 458),\n",
       " (('n', 'o'), 455),\n",
       " (('j', 'o'), 443),\n",
       " (('n', 'y'), 440),\n",
       " (('t', '</E>'), 439),\n",
       " (('s', 's'), 438),\n",
       " (('a', 'c'), 430),\n",
       " (('e', 'v'), 430),\n",
       " (('i', 'k'), 425),\n",
       " (('n', 't'), 421),\n",
       " (('m', 'o'), 418),\n",
       " (('j', 'e'), 412),\n",
       " (('i', 'd'), 408),\n",
       " (('d', 'r'), 407),\n",
       " (('i', 'm'), 405),\n",
       " (('a', 'z'), 402),\n",
       " (('i', 'g'), 399),\n",
       " (('r', 'r'), 392),\n",
       " (('y', 's'), 383),\n",
       " (('<S>', 'f'), 383),\n",
       " (('u', 'r'), 374),\n",
       " (('r', 'l'), 365),\n",
       " (('e', 'd'), 363),\n",
       " (('<S>', 'v'), 363),\n",
       " (('d', 'o'), 361),\n",
       " (('a', 'u'), 359),\n",
       " (('<S>', 'o'), 358),\n",
       " (('z', 'e'), 357),\n",
       " (('k', '</E>'), 356),\n",
       " (('g', 'h'), 342),\n",
       " (('t', 't'), 340),\n",
       " (('z', 'i'), 337),\n",
       " (('k', 'y'), 337),\n",
       " (('t', 'r'), 336),\n",
       " (('c', 'o'), 334),\n",
       " (('t', 'y'), 312),\n",
       " (('g', 'e'), 311),\n",
       " (('b', 'a'), 302),\n",
       " (('k', 'o'), 297),\n",
       " (('u', 'l'), 293),\n",
       " (('g', 'a'), 292),\n",
       " (('c', 'k'), 289),\n",
       " (('l', 'u'), 288),\n",
       " (('y', 'e'), 287),\n",
       " (('k', 'h'), 285),\n",
       " (('y', 'r'), 278),\n",
       " (('m', 'y'), 274),\n",
       " (('i', 'z'), 269),\n",
       " (('d', 'y'), 268),\n",
       " (('c', 'i'), 268),\n",
       " (('n', 's'), 265),\n",
       " (('h', 'o'), 265),\n",
       " (('<S>', 'w'), 262),\n",
       " (('u', 'n'), 256),\n",
       " (('y', 'o'), 256),\n",
       " (('e', 'o'), 255),\n",
       " (('s', 'l'), 255),\n",
       " (('i', 'v'), 251),\n",
       " (('o', 'u'), 251),\n",
       " (('n', 'g'), 249),\n",
       " (('y', 'd'), 244),\n",
       " (('w', 'a'), 243),\n",
       " (('o', 'm'), 240),\n",
       " (('f', 'a'), 237),\n",
       " (('r', 'u'), 235),\n",
       " (('b', 'i'), 198),\n",
       " (('h', 'r'), 197),\n",
       " (('p', 'a'), 195),\n",
       " (('q', 'u'), 195),\n",
       " (('r', 't'), 192),\n",
       " (('h', 'y'), 191),\n",
       " (('s', 'y'), 191),\n",
       " (('y', 'i'), 189),\n",
       " (('p', 'h'), 188),\n",
       " (('n', 'c'), 184),\n",
       " (('j', 'u'), 183),\n",
       " (('g', 'r'), 183),\n",
       " (('g', 'i'), 179),\n",
       " (('r', 'd'), 174),\n",
       " (('e', 'k'), 174),\n",
       " (('h', 'l'), 174),\n",
       " (('s', 'u'), 174),\n",
       " (('n', 'l'), 173),\n",
       " (('e', 'z'), 170),\n",
       " (('a', 'j'), 169),\n",
       " (('p', 'e'), 168),\n",
       " (('r', 's'), 167),\n",
       " (('o', 'v'), 167),\n",
       " (('o', 'd'), 166),\n",
       " (('a', 'x'), 165),\n",
       " (('u', 'a'), 162),\n",
       " (('o', 'h'), 160),\n",
       " (('u', 'e'), 156),\n",
       " (('a', 'g'), 155),\n",
       " (('h', 'u'), 155),\n",
       " (('z', '</E>'), 154),\n",
       " (('m', 'm'), 153),\n",
       " (('a', 'w'), 151),\n",
       " (('f', 'i'), 147),\n",
       " (('r', 'm'), 147),\n",
       " (('u', 'm'), 146),\n",
       " (('v', 'o'), 146),\n",
       " (('u', '</E>'), 143),\n",
       " (('e', 'h'), 143),\n",
       " (('y', 'm'), 141),\n",
       " (('n', 'z'), 140),\n",
       " (('z', 'y'), 140),\n",
       " (('p', 'r'), 139),\n",
       " (('d', 'd'), 138),\n",
       " (('e', 'c'), 138),\n",
       " (('y', 'u'), 136),\n",
       " (('m', 'u'), 136),\n",
       " (('h', 'n'), 135),\n",
       " (('w', 'i'), 133),\n",
       " (('a', 'f'), 132),\n",
       " (('o', 'a'), 132),\n",
       " (('r', 'n'), 131),\n",
       " (('<S>', 'x'), 131),\n",
       " (('l', 'd'), 130),\n",
       " (('w', 'e'), 128),\n",
       " (('u', 'd'), 126),\n",
       " (('z', 'l'), 123),\n",
       " (('k', 'l'), 123),\n",
       " (('x', '</E>'), 121),\n",
       " (('e', 'x'), 120),\n",
       " (('o', 'b'), 120),\n",
       " (('d', 'h'), 116),\n",
       " (('e', 'b'), 116),\n",
       " (('f', 'e'), 115),\n",
       " (('o', 'e'), 115),\n",
       " (('v', 'y'), 113),\n",
       " (('u', 'i'), 113),\n",
       " (('t', 'l'), 112),\n",
       " (('h', 'm'), 111),\n",
       " (('i', 'u'), 111),\n",
       " (('i', 'b'), 110),\n",
       " (('b', '</E>'), 110),\n",
       " (('j', 'i'), 109),\n",
       " (('f', 'r'), 108),\n",
       " (('m', 'b'), 107),\n",
       " (('o', 'c'), 107),\n",
       " (('c', 'l'), 106),\n",
       " (('o', 't'), 105),\n",
       " (('r', 'h'), 105),\n",
       " (('z', 'o'), 105),\n",
       " (('u', 'b'), 104),\n",
       " (('e', 'g'), 104),\n",
       " (('t', 'z'), 104),\n",
       " (('y', 'c'), 103),\n",
       " (('y', 'v'), 102),\n",
       " (('g', '</E>'), 101),\n",
       " (('o', 'o'), 101),\n",
       " (('k', 'r'), 100),\n",
       " (('x', 'a'), 100),\n",
       " (('x', 'i'), 97),\n",
       " (('u', 'c'), 97),\n",
       " (('o', 'y'), 96),\n",
       " (('i', 'f'), 94),\n",
       " (('c', '</E>'), 94),\n",
       " (('i', 'h'), 94),\n",
       " (('b', 'o'), 92),\n",
       " (('k', 's'), 92),\n",
       " (('y', 't'), 92),\n",
       " (('n', 'u'), 91),\n",
       " (('r', 'c'), 91),\n",
       " (('s', 'm'), 91),\n",
       " (('d', 'u'), 90),\n",
       " (('o', 'w'), 90),\n",
       " (('l', 's'), 87),\n",
       " (('u', 'k'), 87),\n",
       " (('o', 'p'), 87),\n",
       " (('v', '</E>'), 86),\n",
       " (('c', 'y'), 86),\n",
       " (('m', 'r'), 85),\n",
       " (('<S>', 'q'), 85),\n",
       " (('b', 'l'), 83),\n",
       " (('r', 'k'), 81),\n",
       " (('i', 'i'), 80),\n",
       " (('e', 'f'), 80),\n",
       " (('y', 'z'), 79),\n",
       " (('f', '</E>'), 78),\n",
       " (('g', 'u'), 78),\n",
       " (('y', 'k'), 78),\n",
       " (('g', 'o'), 77),\n",
       " (('a', 'p'), 77),\n",
       " (('e', 'p'), 77),\n",
       " (('s', 'k'), 76),\n",
       " (('r', 'v'), 75),\n",
       " (('i', 'j'), 74),\n",
       " (('u', 't'), 74),\n",
       " (('r', 'g'), 74),\n",
       " (('l', 't'), 73),\n",
       " (('i', 'x'), 71),\n",
       " (('<S>', 'u'), 71),\n",
       " (('l', 'v'), 69),\n",
       " (('z', 'u'), 69),\n",
       " (('c', 'r'), 69),\n",
       " (('e', 'u'), 68),\n",
       " (('t', 'u'), 68),\n",
       " (('j', '</E>'), 66),\n",
       " (('o', 'i'), 66),\n",
       " (('b', 'd'), 65),\n",
       " (('b', 'y'), 64),\n",
       " (('u', 'w'), 64),\n",
       " (('w', 'y'), 63),\n",
       " (('x', 't'), 63),\n",
       " (('a', 'o'), 62),\n",
       " (('o', 'k'), 61),\n",
       " (('l', 'm'), 59),\n",
       " (('a', 'q'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('p', 'o'), 58),\n",
       " (('h', 't'), 56),\n",
       " (('e', 'j'), 56),\n",
       " (('s', 'r'), 55),\n",
       " (('p', 'i'), 55),\n",
       " (('n', 'v'), 54),\n",
       " (('f', 'o'), 54),\n",
       " (('w', 'n'), 54),\n",
       " (('o', 'z'), 54),\n",
       " (('s', 'c'), 53),\n",
       " (('n', 'k'), 51),\n",
       " (('d', 'l'), 50),\n",
       " (('i', 'q'), 49),\n",
       " (('i', 'p'), 49),\n",
       " (('k', 'u'), 48),\n",
       " (('l', 'b'), 46),\n",
       " (('s', 'p'), 46),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('v', 'r'), 44),\n",
       " (('m', 'c'), 44),\n",
       " (('e', 'w'), 43),\n",
       " (('u', 'g'), 43),\n",
       " (('u', 'z'), 43),\n",
       " (('b', 'h'), 41),\n",
       " (('f', 'f'), 41),\n",
       " (('z', 'z'), 41),\n",
       " (('n', 'r'), 40),\n",
       " (('h', 'v'), 39),\n",
       " (('w', '</E>'), 39),\n",
       " (('c', 'c'), 38),\n",
       " (('z', 'h'), 38),\n",
       " (('o', 'g'), 37),\n",
       " (('o', 'x'), 37),\n",
       " (('w', 'o'), 36),\n",
       " (('u', 'v'), 36),\n",
       " (('x', 'o'), 36),\n",
       " (('b', 'b'), 35),\n",
       " (('p', 'p'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('x', 'e'), 34),\n",
       " (('c', 'u'), 34),\n",
       " (('x', 'x'), 34),\n",
       " (('t', 's'), 34),\n",
       " (('c', 't'), 33),\n",
       " (('p', '</E>'), 33),\n",
       " (('o', 'f'), 33),\n",
       " (('r', 'b'), 33),\n",
       " (('k', 'w'), 32),\n",
       " (('x', 'l'), 32),\n",
       " (('m', 'p'), 32),\n",
       " (('h', 'k'), 31),\n",
       " (('m', 's'), 30),\n",
       " (('h', 's'), 29),\n",
       " (('d', 'm'), 29),\n",
       " (('x', 's'), 29),\n",
       " (('g', 'l'), 29),\n",
       " (('q', '</E>'), 28),\n",
       " (('x', 'y'), 28),\n",
       " (('g', 'y'), 28),\n",
       " (('y', 'b'), 28),\n",
       " (('y', 'g'), 27),\n",
       " (('d', 'n'), 27),\n",
       " (('u', 'x'), 27),\n",
       " (('l', 'c'), 26),\n",
       " (('g', 'w'), 26),\n",
       " (('g', 'n'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('z', 'r'), 25),\n",
       " (('n', 'h'), 24),\n",
       " (('d', 's'), 24),\n",
       " (('l', 'k'), 23),\n",
       " (('h', 'd'), 23),\n",
       " (('w', 'u'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('g', 's'), 23),\n",
       " (('d', 'g'), 23),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'z'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('d', 'w'), 22),\n",
       " (('y', 'j'), 22),\n",
       " (('g', 't'), 22),\n",
       " (('s', 'n'), 22),\n",
       " (('k', 'k'), 21),\n",
       " (('r', 'w'), 21),\n",
       " (('g', 'g'), 21),\n",
       " (('s', 'w'), 21),\n",
       " (('k', 'n'), 21),\n",
       " (('h', 'z'), 20),\n",
       " (('m', 'd'), 20),\n",
       " (('w', 'h'), 20),\n",
       " (('x', 'z'), 19),\n",
       " (('s', 'b'), 19),\n",
       " (('g', 'd'), 19),\n",
       " (('t', 'n'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('l', 'r'), 18),\n",
       " (('l', 'h'), 18),\n",
       " (('y', 'x'), 18),\n",
       " (('f', 'l'), 18),\n",
       " (('w', 'r'), 18),\n",
       " (('d', 'v'), 17),\n",
       " (('m', 'n'), 17),\n",
       " (('n', 'm'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('l', 'w'), 16),\n",
       " (('f', 't'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('w', 's'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('p', 't'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('t', 'c'), 15),\n",
       " (('l', 'p'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('p', 's'), 14),\n",
       " (('y', 'p'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('n', 'f'), 13),\n",
       " (('p', 'l'), 13),\n",
       " (('v', 'l'), 13),\n",
       " (('e', 'q'), 13),\n",
       " (('u', 'p'), 13),\n",
       " (('q', 'i'), 12),\n",
       " (('q', 'a'), 12),\n",
       " (('n', 'w'), 12),\n",
       " (('r', 'p'), 12),\n",
       " (('w', 'l'), 12),\n",
       " (('u', 'y'), 12),\n",
       " (('l', 'n'), 12),\n",
       " (('f', 'y'), 12),\n",
       " (('m', 'z'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('y', 'f'), 11),\n",
       " (('f', 'u'), 11),\n",
       " (('c', 'q'), 11),\n",
       " (('h', 'w'), 10),\n",
       " (('p', 'y'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('d', 'j'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('b', 's'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('s', 'd'), 8),\n",
       " (('v', 'n'), 8),\n",
       " (('v', 'u'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('j', 'r'), 8),\n",
       " (('j', 's'), 7),\n",
       " (('u', 'o'), 7),\n",
       " (('w', 'd'), 7),\n",
       " (('l', 'z'), 7),\n",
       " (('v', 'v'), 7),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('f', 's'), 6),\n",
       " (('m', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('h', 'b'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('m', 'l'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('c', 's'), 5),\n",
       " (('x', 'u'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('b', 'n'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('z', 'b'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('n', 'x'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('h', 'g'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('l', 'q'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('j', 'n'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('v', 'k'), 3),\n",
       " (('k', 'b'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('b', 'j'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('t', 'b'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('j', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('x', 'w'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('g', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('p', 'b'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('k', 'c'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('t', 'g'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z', 'z', 'y', 'z', 'x']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zzyzx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zyzx'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the information in 2d array - rows = 1st charachter, columns = 2nd charachter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3, 5), dtype = torch.int32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing bit in the array\n",
    "a[1, 3] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(''.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " # stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(27):\n",
    "  for j in range(27):\n",
    "    chstr = itos[i] + itos[j]\n",
    "    plt.text(j, i, chstr, ha='center', va='bottom', color= 'gray')\n",
    "    plt.text(j, i, N[i, j].item(), ha='center', va='top', color= 'gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4155, 1153, 1398, 1579, 1435,  383,  620,  798,  553, 2239, 2700,\n",
       "        1451, 2401, 1090,  358,  467,   85, 1471, 1914, 1209,   71,  363,  262,\n",
       "         131,  519,  876], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1400, 0.0388, 0.0471, 0.0532, 0.0483, 0.0129, 0.0209, 0.0269,\n",
       "        0.0186, 0.0754, 0.0910, 0.0489, 0.0809, 0.0367, 0.0121, 0.0157, 0.0029,\n",
       "        0.0496, 0.0645, 0.0407, 0.0024, 0.0122, 0.0088, 0.0044, 0.0175, 0.0295])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating probability vector\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6064, 0.3033, 0.0903])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n",
       "        0, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(p, num_samples=100, replacement=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27, 27\n",
    "# 27, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.sum(1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9531)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Broadcasting Sementics from Pytorch or tutorials and practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    # p = N[ix].float()\n",
    "    # p = p / p.sum()\n",
    "    # p = torch.ones(27) / 27.0\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log(a+b+c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximize likelihood of the data w.r.t model parameters (statistical modeling)\n",
    "# equivalent ot maximizing the log likelihood ( beacause log is the monotonic)\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-522205.6562)\n",
      "nll=tensor(522205.6562)\n",
      "nll/n=tensor(2.4548)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "# for w in ['ebadx']:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    prob = P[ix1, ix2]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "    # print(f'{ch1}{ch2}: {prob:.4f} {logprob:4f}') # probabilities model assigns to the bigrams\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}') \n",
    "print(f'{nll/n=}') # average negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: to see the literature of Maximum likelihood estimation\n",
    "* WolframAlpha website for logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". a\n",
      "a a\n",
      "a b\n",
      "b a\n",
      "a n\n",
      "n .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of all the bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) \n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  2,  1, 14])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  1, 14,  0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding with PyTorch\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # casting this to float\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 27])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#plt.imshow(xenc.T, cmap='Blues')\n",
    "# turned on bits are the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data types have to be float for NN\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3332,  1.0716,  0.1085,  0.1040, -0.6817, -0.7600,  0.1348,  0.0653,\n",
       "          0.8847,  0.7496, -1.0206, -0.4908, -0.5470, -0.6289, -0.3616,  0.4408,\n",
       "          0.5945, -1.4407,  0.2931, -0.5711,  0.1083, -0.8254, -1.1108,  1.2223,\n",
       "          0.0374, -1.4060,  2.0371],\n",
       "        [ 0.0838, -0.7973,  0.0232, -0.7750, -0.2299,  0.2636, -0.8441,  0.0179,\n",
       "          1.0942, -0.2748,  1.5213,  0.1811, -0.4153,  0.5469,  0.1684, -0.0772,\n",
       "          0.5558, -0.2036, -0.1618,  0.7381, -0.8191,  0.9044,  0.3088, -1.0977,\n",
       "         -0.1128,  1.2148,  0.4995],\n",
       "        [ 0.0838, -0.7973,  0.0232, -0.7750, -0.2299,  0.2636, -0.8441,  0.0179,\n",
       "          1.0942, -0.2748,  1.5213,  0.1811, -0.4153,  0.5469,  0.1684, -0.0772,\n",
       "          0.5558, -0.2036, -0.1618,  0.7381, -0.8191,  0.9044,  0.3088, -1.0977,\n",
       "         -0.1128,  1.2148,  0.4995],\n",
       "        [-0.1689,  1.1564, -0.2120,  0.6238, -0.9548,  0.4504, -0.1378, -0.6064,\n",
       "          1.1054,  0.1802, -0.5261, -0.4971,  1.0251,  0.6337, -0.4048,  0.3829,\n",
       "         -0.7017,  0.5356, -0.5768, -2.0160, -0.0182,  0.5113,  0.0317, -1.1724,\n",
       "          2.3606, -0.6001,  0.4781],\n",
       "        [ 0.0838, -0.7973,  0.0232, -0.7750, -0.2299,  0.2636, -0.8441,  0.0179,\n",
       "          1.0942, -0.2748,  1.5213,  0.1811, -0.4153,  0.5469,  0.1684, -0.0772,\n",
       "          0.5558, -0.2036, -0.1618,  0.7381, -0.8191,  0.9044,  0.3088, -1.0977,\n",
       "         -0.1128,  1.2148,  0.4995],\n",
       "        [ 0.1709,  0.6464,  1.0844, -0.4412,  0.2150, -1.3968, -0.2771, -0.0355,\n",
       "          0.1326,  0.2052,  0.0080, -0.5921,  0.8054, -1.3861, -0.4451,  1.4280,\n",
       "         -1.6429,  0.3316,  0.2306, -0.6144, -0.1504,  0.8723, -0.1162,  0.0846,\n",
       "          0.1687,  0.5894, -0.1859]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 27)\n",
    "xenc @ W # @ used for matrix multiplication in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 27])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6337)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3, 13] # firing rate for the 13th neuron when the input is the 3rd neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0199, 0.0809, 0.0309, 0.0307, 0.0140, 0.0130, 0.0317, 0.0296, 0.0671,\n",
       "         0.0586, 0.0100, 0.0170, 0.0160, 0.0148, 0.0193, 0.0430, 0.0502, 0.0066,\n",
       "         0.0371, 0.0156, 0.0309, 0.0121, 0.0091, 0.0940, 0.0288, 0.0068, 0.2124],\n",
       "        [0.0297, 0.0123, 0.0280, 0.0126, 0.0217, 0.0356, 0.0117, 0.0278, 0.0816,\n",
       "         0.0208, 0.1251, 0.0327, 0.0180, 0.0472, 0.0323, 0.0253, 0.0476, 0.0223,\n",
       "         0.0232, 0.0572, 0.0120, 0.0675, 0.0372, 0.0091, 0.0244, 0.0921, 0.0450],\n",
       "        [0.0297, 0.0123, 0.0280, 0.0126, 0.0217, 0.0356, 0.0117, 0.0278, 0.0816,\n",
       "         0.0208, 0.1251, 0.0327, 0.0180, 0.0472, 0.0323, 0.0253, 0.0476, 0.0223,\n",
       "         0.0232, 0.0572, 0.0120, 0.0675, 0.0372, 0.0091, 0.0244, 0.0921, 0.0450],\n",
       "        [0.0201, 0.0758, 0.0193, 0.0445, 0.0092, 0.0374, 0.0208, 0.0130, 0.0720,\n",
       "         0.0286, 0.0141, 0.0145, 0.0665, 0.0449, 0.0159, 0.0350, 0.0118, 0.0407,\n",
       "         0.0134, 0.0032, 0.0234, 0.0398, 0.0246, 0.0074, 0.2527, 0.0131, 0.0385],\n",
       "        [0.0297, 0.0123, 0.0280, 0.0126, 0.0217, 0.0356, 0.0117, 0.0278, 0.0816,\n",
       "         0.0208, 0.1251, 0.0327, 0.0180, 0.0472, 0.0323, 0.0253, 0.0476, 0.0223,\n",
       "         0.0232, 0.0572, 0.0120, 0.0675, 0.0372, 0.0091, 0.0244, 0.0921, 0.0450],\n",
       "        [0.0353, 0.0569, 0.0881, 0.0192, 0.0369, 0.0074, 0.0226, 0.0287, 0.0340,\n",
       "         0.0366, 0.0300, 0.0165, 0.0667, 0.0074, 0.0191, 0.1242, 0.0058, 0.0415,\n",
       "         0.0375, 0.0161, 0.0256, 0.0713, 0.0265, 0.0324, 0.0353, 0.0537, 0.0247]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # all the negative values are converted to positive values by using exp and all the positive values are increased. Equivalent to the N matrix.\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0199, 0.0809, 0.0309, 0.0307, 0.0140, 0.0130, 0.0317, 0.0296, 0.0671,\n",
       "        0.0586, 0.0100, 0.0170, 0.0160, 0.0148, 0.0193, 0.0430, 0.0502, 0.0066,\n",
       "        0.0371, 0.0156, 0.0309, 0.0121, 0.0091, 0.0940, 0.0288, 0.0068, 0.2124])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 27])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6289,  0.5469,  0.6337, -1.4847,  0.7977, -0.4745,  0.2883,  2.3038,\n",
       "         0.4826,  1.2167,  0.1630, -0.6329,  0.8956,  1.4039, -1.3861, -0.1797,\n",
       "         1.0911, -0.2216,  1.9568,  0.0128, -0.1016, -0.2015,  0.4741, -0.7818,\n",
       "         0.9654,  0.6171,  0.3163])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6337)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum() # Wx dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  2,  1, 14])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  1, 14,  0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialized 27 neurons weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27, 27, generator=g) # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # one hot encoding of the inputs\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts equivalent to N matrix\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# The last 2  lines together are called softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 27])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "bigram example 1: .a (indexes: 0, 1)\n",
      "input to the neural network: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "output probabilities from the neural network: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 1\n",
      "probability assigned by the network to the correct character: 0.009982486255466938\n",
      "log likelihood: -4.6069231033325195\n",
      "negative log likelihood: -4.6069231033325195\n",
      "=======================\n",
      "average negative log likelihood, i.e loss = -0.9213846325874329\n",
      "----------------------\n",
      "bigram example 2: aa (indexes: 1, 1)\n",
      "input to the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "output probabilities from the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 1\n",
      "probability assigned by the network to the correct character: 0.008642823435366154\n",
      "log likelihood: -4.751026153564453\n",
      "negative log likelihood: -4.751026153564453\n",
      "=======================\n",
      "average negative log likelihood, i.e loss = -1.8715898990631104\n",
      "----------------------\n",
      "bigram example 3: ab (indexes: 1, 2)\n",
      "input to the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "output probabilities from the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 2\n",
      "probability assigned by the network to the correct character: 0.03963678330183029\n",
      "log likelihood: -3.2279977798461914\n",
      "negative log likelihood: -3.2279977798461914\n",
      "=======================\n",
      "average negative log likelihood, i.e loss = -2.5171895027160645\n",
      "----------------------\n",
      "bigram example 4: ba (indexes: 2, 1)\n",
      "input to the neural network: tensor([0.0249, 0.0079, 0.0314, 0.0344, 0.0086, 0.0098, 0.0246, 0.0174, 0.0076,\n",
      "        0.0046, 0.0548, 0.0089, 0.1230, 0.1814, 0.0091, 0.0369, 0.0398, 0.0977,\n",
      "        0.0075, 0.0110, 0.0104, 0.1236, 0.0256, 0.0579, 0.0115, 0.0232, 0.0067])\n",
      "output probabilities from the neural network: tensor([0.0249, 0.0079, 0.0314, 0.0344, 0.0086, 0.0098, 0.0246, 0.0174, 0.0076,\n",
      "        0.0046, 0.0548, 0.0089, 0.1230, 0.1814, 0.0091, 0.0369, 0.0398, 0.0977,\n",
      "        0.0075, 0.0110, 0.0104, 0.1236, 0.0256, 0.0579, 0.0115, 0.0232, 0.0067])\n",
      "label (actual next character): 1\n",
      "probability assigned by the network to the correct character: 0.007889186032116413\n",
      "log likelihood: -4.842262268066406\n",
      "negative log likelihood: -4.842262268066406\n",
      "=======================\n",
      "average negative log likelihood, i.e loss = -3.4856419563293457\n",
      "----------------------\n",
      "bigram example 5: an (indexes: 1, 14)\n",
      "input to the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "output probabilities from the neural network: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 14\n",
      "probability assigned by the network to the correct character: 0.02320818044245243\n",
      "log likelihood: -3.7632503509521484\n",
      "negative log likelihood: -3.7632503509521484\n",
      "=======================\n",
      "average negative log likelihood, i.e loss = -4.2382917404174805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('----------------------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes: {x}, {y})')\n",
    "  print('input to the neural network:', probs[i])\n",
    "  print('output probabilities from the neural network:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print( 'probability assigned by the network to the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "  print('=======================')\n",
    "  print('average negative log likelihood, i.e loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- optimization ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  2,  1, 14])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  1, 14,  0])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialized 27 neurons weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27, 27, generator=g, requires_grad=True) # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # one hot encoding of the inputs\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts equivalent to N matrix\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(6), ys].log().mean() # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 27])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs[0, 1], probs[1, 1], probs[2, 2], probs[3, 1], probs[4, 14], probs[5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.arange(6) # with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Pytorch\n",
    "#loss = -probs[torch.arange(6), ys].log().mean()\n",
    "#loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass in Pytorch\n",
    "W.grad = None # setting the gradients to zero\n",
    "loss.backward() # backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.791404962539673\n"
     ]
    }
   ],
   "source": [
    "print(loss.item()) # loss getting smaller by the backward, update and forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the tensor\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------Rearranging the code--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 212725\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words: #[:1] # optimizing the entire training set\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)\n",
    "\n",
    "# initialize the 'networs'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27, 27, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46226167678833\n",
      "2.4622151851654053\n",
      "2.4621694087982178\n",
      "2.4621241092681885\n",
      "2.4620792865753174\n",
      "2.4620349407196045\n",
      "2.4619908332824707\n",
      "2.4619476795196533\n",
      "2.4619040489196777\n",
      "2.461862087249756\n",
      "2.461819887161255\n",
      "2.461778163909912\n",
      "2.4617369174957275\n",
      "2.461696147918701\n",
      "2.461655855178833\n",
      "2.461616039276123\n",
      "2.461576223373413\n",
      "2.4615371227264404\n",
      "2.461498498916626\n",
      "2.4614598751068115\n",
      "2.4614217281341553\n",
      "2.4613840579986572\n",
      "2.4613468647003174\n",
      "2.4613096714019775\n",
      "2.461273431777954\n",
      "2.4612371921539307\n",
      "2.4612011909484863\n",
      "2.4611656665802\n",
      "2.461130380630493\n",
      "2.4610955715179443\n",
      "2.4610610008239746\n",
      "2.461026668548584\n",
      "2.4609928131103516\n",
      "2.4609594345092773\n",
      "2.460926055908203\n",
      "2.460893154144287\n",
      "2.46086049079895\n",
      "2.4608280658721924\n",
      "2.4607958793640137\n",
      "2.460763931274414\n",
      "2.4607326984405518\n",
      "2.4607014656066895\n",
      "2.4606704711914062\n",
      "2.460639715194702\n",
      "2.4606094360351562\n",
      "2.4605791568756104\n",
      "2.4605493545532227\n",
      "2.460519552230835\n",
      "2.4604902267456055\n",
      "2.460461139678955\n",
      "2.4604320526123047\n",
      "2.4604034423828125\n",
      "2.4603753089904785\n",
      "2.4603469371795654\n",
      "2.4603190422058105\n",
      "2.460291624069214\n",
      "2.460264205932617\n",
      "2.4602367877960205\n",
      "2.460209846496582\n",
      "2.4601831436157227\n",
      "2.460156202316284\n",
      "2.460130214691162\n",
      "2.460103988647461\n",
      "2.460078001022339\n",
      "2.460052251815796\n",
      "2.460026979446411\n",
      "2.4600017070770264\n",
      "2.4599766731262207\n",
      "2.459951877593994\n",
      "2.4599270820617676\n",
      "2.45990252494812\n",
      "2.4598782062530518\n",
      "2.4598538875579834\n",
      "2.4598302841186523\n",
      "2.459806442260742\n",
      "2.459782838821411\n",
      "2.459759473800659\n",
      "2.4597363471984863\n",
      "2.4597132205963135\n",
      "2.459690570831299\n",
      "2.459667444229126\n",
      "2.4596452713012695\n",
      "2.459622859954834\n",
      "2.4596006870269775\n",
      "2.4595789909362793\n",
      "2.459557056427002\n",
      "2.4595351219177246\n",
      "2.4595136642456055\n",
      "2.4594924449920654\n",
      "2.4594714641571045\n",
      "2.4594502449035645\n",
      "2.4594290256500244\n",
      "2.459408760070801\n",
      "2.459388256072998\n",
      "2.459367513656616\n",
      "2.4593472480773926\n",
      "2.459327459335327\n",
      "2.4593071937561035\n",
      "2.459287166595459\n",
      "2.4592676162719727\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100): # iterations\n",
    "\n",
    "  # Forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # one hot encoding of the inputs\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts equivalent to N matrix\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() # negative log likelihood\n",
    "  print(loss.item())\n",
    "\n",
    "  # Backward pass\n",
    "  W.grad = None # setting the gradients to zero\n",
    "  loss.backward() # backpropagation\n",
    "\n",
    "  # Update\n",
    "  W.data += -50 * W.grad\n",
    "\n",
    "  # entire training set is optimized in one go (achieveing the same result as the goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

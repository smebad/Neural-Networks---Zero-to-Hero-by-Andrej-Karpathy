{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - with using paper Bengio et al.2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaban', 'aabid', 'aabidah', 'aabir', 'aabriella', 'aada', 'aadam', 'aadarsh']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('cleaned_names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29681"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaban\n",
      "... ----> a\n",
      "..a ----> a\n",
      ".aa ----> b\n",
      "aab ----> a\n",
      "aba ----> n\n",
      "ban ----> .\n",
      "aabid\n",
      "... ----> a\n",
      "..a ----> a\n",
      ".aa ----> b\n",
      "aab ----> i\n",
      "abi ----> d\n",
      "bid ----> .\n",
      "aabidah\n",
      "... ----> a\n",
      "..a ----> a\n",
      ".aa ----> b\n",
      "aab ----> i\n",
      "abi ----> d\n",
      "bid ----> a\n",
      "ida ----> h\n",
      "dah ----> .\n",
      "aabir\n",
      "... ----> a\n",
      "..a ----> a\n",
      ".aa ----> b\n",
      "aab ----> i\n",
      "abi ----> r\n",
      "bir ----> .\n",
      "aabriella\n",
      "... ----> a\n",
      "..a ----> a\n",
      ".aa ----> b\n",
      "aab ----> r\n",
      "abr ----> i\n",
      "bri ----> e\n",
      "rie ----> l\n",
      "iel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters to use to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "\n",
    "  print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    print(''.join([itos[i] for i in context]), '---->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 3]), torch.int64, torch.Size([36]), torch.int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  1,  2],\n",
       "        [ 1,  2,  1],\n",
       "        [ 2,  1, 14],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  1,  2],\n",
       "        [ 1,  2,  9],\n",
       "        [ 2,  9,  4],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  1,  2],\n",
       "        [ 1,  2,  9],\n",
       "        [ 2,  9,  4],\n",
       "        [ 9,  4,  1],\n",
       "        [ 4,  1,  8],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  1,  2],\n",
       "        [ 1,  2,  9],\n",
       "        [ 2,  9, 18],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  1,  2],\n",
       "        [ 1,  2, 18],\n",
       "        [ 2, 18,  9],\n",
       "        [18,  9,  5],\n",
       "        [ 9,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  1, 14,  0,  1,  1,  2,  9,  4,  0,  1,  1,  2,  9,  4,  1,\n",
       "         8,  0,  1,  1,  2,  9, 18,  0,  1,  1,  2, 18,  9,  5, 12, 12,  1,  0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(27, 2) # each 27 characters is represented by a 2D embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2267, -1.1814],\n",
       "        [-0.3037,  0.5494],\n",
       "        [ 0.7256, -0.4971],\n",
       "        [ 0.3972,  0.0945],\n",
       "        [ 0.1055, -0.1769],\n",
       "        [ 0.1621,  0.5746],\n",
       "        [-0.8591,  0.1463],\n",
       "        [ 0.6487,  0.3895],\n",
       "        [-1.7086, -0.1245],\n",
       "        [-0.4339, -0.5667],\n",
       "        [-0.2502,  0.4662],\n",
       "        [ 0.5453,  0.7484],\n",
       "        [ 0.6187,  0.5553],\n",
       "        [ 0.7460, -0.3221],\n",
       "        [-1.0132, -1.2370],\n",
       "        [ 0.7007,  0.9348],\n",
       "        [-0.0927,  0.2925],\n",
       "        [-1.5795, -0.6300],\n",
       "        [-0.1514,  0.9762],\n",
       "        [ 0.1321,  1.8256],\n",
       "        [-0.2423, -0.5703],\n",
       "        [-0.6588,  0.1696],\n",
       "        [ 0.4282,  1.7926],\n",
       "        [ 0.7237, -0.1539],\n",
       "        [-0.5655,  0.5998],\n",
       "        [ 0.1590, -0.8350],\n",
       "        [-0.5081,  1.6276]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1621, 0.5746])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1621, 0.5746])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C # one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1621,  0.5746],\n",
       "        [-0.8591,  0.1463],\n",
       "        [ 0.6487,  0.3895]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,6,7]] # indexing with list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1621,  0.5746],\n",
       "        [-0.8591,  0.1463],\n",
       "        [ 0.6487,  0.3895]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5,6,7])] # indexing with tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 3, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2] # 13th example, 2nd character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3037,  0.5494])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[13,2]] # embedding of the 13th example, 2nd character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3037,  0.5494])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 3, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the hidden layer\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = emb.view(-1, 6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1537, -3.8829, -0.1728,  ...,  1.3942, -4.0677,  1.2994],\n",
       "        [ 2.7803, -5.2128,  0.3832,  ...,  1.6790,  0.8570, -0.4494],\n",
       "        [ 0.7629, -3.2683, -0.3648,  ...,  0.6772,  1.1944, -0.1140],\n",
       "        ...,\n",
       "        [ 0.2591,  1.8355, -0.4823,  ...,  0.3382,  0.9148, -1.9791],\n",
       "        [ 1.6234,  1.0837, -0.5028,  ...,  0.4975, -0.2569, -2.4144],\n",
       "        [ 2.0655, -3.0583, -0.4778,  ...,  0.3471, -0.1510, -0.9035]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 6])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape # first, second, third characters of the first example concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 6])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat(torch.unbind(emb, 1), 1).shape # split the tensor along the second dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9) # view as 2x9 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage() # memory address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 3, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9964, -0.9992, -0.1711,  ...,  0.8841, -0.9994,  0.8616],\n",
       "        [ 0.9923, -0.9999,  0.3655,  ...,  0.9327,  0.6947, -0.4214],\n",
       "        [ 0.6428, -0.9971, -0.3494,  ...,  0.5897,  0.8319, -0.1135],\n",
       "        ...,\n",
       "        [ 0.2535,  0.9504, -0.4481,  ...,  0.3259,  0.7234, -0.9625],\n",
       "        [ 0.9251,  0.7946, -0.4643,  ...,  0.4601, -0.2514, -0.9841],\n",
       "        [ 0.9684, -0.9956, -0.4445,  ...,  0.3338, -0.1499, -0.7180]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 100])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 100])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(-1, 6) @ W1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32, 100\n",
    "# broadcasting to 100\n",
    "# 1 , 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final layer (output layer)\n",
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 27])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 27])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.8065e-09, 1.4690e-04, 1.9862e-05, 3.8407e-08, 7.6152e-08, 7.1989e-12,\n",
       "         9.5434e-11, 2.1641e-07, 5.3638e-09, 2.0643e-08, 6.4877e-10, 1.4118e-02,\n",
       "         4.4283e-11, 2.8704e-05, 3.8206e-03, 1.4188e-10, 7.2322e-01, 2.4760e-08,\n",
       "         1.4033e-04, 4.8933e-08, 8.5709e-07, 1.9782e-06, 9.9065e-08, 9.2512e-10,\n",
       "         2.5850e-01, 3.4688e-07, 1.8002e-10],\n",
       "        [5.4459e-14, 1.2444e-08, 3.2858e-06, 9.8884e-06, 1.7350e-11, 1.9971e-09,\n",
       "         2.8320e-07, 1.2003e-12, 2.7561e-10, 6.1739e-12, 7.5947e-04, 5.9870e-06,\n",
       "         2.8088e-09, 5.1304e-07, 7.5357e-07, 2.0542e-08, 3.2429e-02, 6.0873e-12,\n",
       "         2.6353e-04, 7.4122e-06, 2.2237e-08, 9.4560e-01, 9.5726e-11, 1.0953e-14,\n",
       "         2.0924e-02, 7.3264e-09, 3.1278e-10],\n",
       "        [7.0010e-04, 2.0603e-13, 1.5530e-06, 1.0714e-01, 1.3761e-02, 1.2612e-04,\n",
       "         7.6711e-10, 3.4976e-10, 3.8315e-03, 3.6186e-10, 1.4581e-02, 3.2739e-09,\n",
       "         3.2324e-08, 2.5105e-04, 3.8956e-04, 5.9131e-01, 8.0743e-06, 1.8042e-07,\n",
       "         7.6603e-02, 4.1685e-12, 9.1059e-05, 1.8683e-01, 8.9454e-07, 3.2527e-08,\n",
       "         3.8128e-03, 5.6180e-04, 1.6463e-06],\n",
       "        [3.3596e-09, 7.8607e-14, 5.9110e-12, 3.6200e-14, 5.8699e-06, 5.7815e-13,\n",
       "         1.5543e-11, 1.5396e-06, 3.5234e-10, 4.9895e-09, 2.1321e-10, 1.3153e-10,\n",
       "         3.2735e-16, 1.7703e-06, 1.3779e-07, 1.1539e-01, 1.4661e-12, 2.4710e-15,\n",
       "         4.6268e-11, 1.0071e-07, 8.8460e-01, 3.0025e-13, 6.1888e-13, 3.4303e-06,\n",
       "         1.5371e-09, 1.2719e-12, 2.9517e-14],\n",
       "        [2.1257e-10, 7.0056e-14, 4.2984e-09, 2.5142e-07, 4.6666e-10, 2.9776e-11,\n",
       "         4.2005e-07, 8.5661e-14, 2.7253e-11, 2.6613e-10, 2.7888e-05, 5.7396e-08,\n",
       "         6.3415e-15, 9.3875e-15, 2.6853e-13, 1.1164e-12, 1.1961e-10, 2.2166e-18,\n",
       "         4.2970e-10, 9.9800e-01, 1.9291e-03, 1.8806e-06, 3.5704e-13, 9.8475e-12,\n",
       "         3.9706e-05, 2.1119e-13, 1.6158e-11],\n",
       "        [1.3313e-03, 6.1594e-09, 4.3576e-07, 1.4425e-08, 1.9251e-01, 4.8356e-13,\n",
       "         3.8602e-17, 2.9789e-10, 1.3517e-04, 7.4573e-09, 1.0338e-12, 9.9766e-08,\n",
       "         8.1147e-12, 1.4635e-02, 2.4420e-03, 2.5479e-02, 1.0605e-05, 3.3175e-09,\n",
       "         2.4809e-08, 5.9187e-11, 5.1130e-06, 2.5141e-04, 1.9486e-10, 1.1009e-03,\n",
       "         7.6210e-01, 6.0982e-07, 1.0012e-12],\n",
       "        [8.8065e-09, 1.4690e-04, 1.9862e-05, 3.8407e-08, 7.6152e-08, 7.1989e-12,\n",
       "         9.5434e-11, 2.1641e-07, 5.3638e-09, 2.0643e-08, 6.4877e-10, 1.4118e-02,\n",
       "         4.4283e-11, 2.8704e-05, 3.8206e-03, 1.4188e-10, 7.2322e-01, 2.4760e-08,\n",
       "         1.4033e-04, 4.8933e-08, 8.5709e-07, 1.9782e-06, 9.9065e-08, 9.2512e-10,\n",
       "         2.5850e-01, 3.4688e-07, 1.8002e-10],\n",
       "        [5.4459e-14, 1.2444e-08, 3.2858e-06, 9.8884e-06, 1.7350e-11, 1.9971e-09,\n",
       "         2.8320e-07, 1.2003e-12, 2.7561e-10, 6.1739e-12, 7.5947e-04, 5.9870e-06,\n",
       "         2.8088e-09, 5.1304e-07, 7.5357e-07, 2.0542e-08, 3.2429e-02, 6.0873e-12,\n",
       "         2.6353e-04, 7.4122e-06, 2.2237e-08, 9.4560e-01, 9.5726e-11, 1.0953e-14,\n",
       "         2.0924e-02, 7.3264e-09, 3.1278e-10],\n",
       "        [7.0010e-04, 2.0603e-13, 1.5530e-06, 1.0714e-01, 1.3761e-02, 1.2612e-04,\n",
       "         7.6711e-10, 3.4976e-10, 3.8315e-03, 3.6186e-10, 1.4581e-02, 3.2739e-09,\n",
       "         3.2324e-08, 2.5105e-04, 3.8956e-04, 5.9131e-01, 8.0743e-06, 1.8042e-07,\n",
       "         7.6603e-02, 4.1685e-12, 9.1059e-05, 1.8683e-01, 8.9454e-07, 3.2527e-08,\n",
       "         3.8128e-03, 5.6180e-04, 1.6463e-06],\n",
       "        [3.3596e-09, 7.8607e-14, 5.9110e-12, 3.6200e-14, 5.8699e-06, 5.7815e-13,\n",
       "         1.5543e-11, 1.5396e-06, 3.5234e-10, 4.9895e-09, 2.1321e-10, 1.3153e-10,\n",
       "         3.2735e-16, 1.7703e-06, 1.3779e-07, 1.1539e-01, 1.4661e-12, 2.4710e-15,\n",
       "         4.6268e-11, 1.0071e-07, 8.8460e-01, 3.0025e-13, 6.1888e-13, 3.4303e-06,\n",
       "         1.5371e-09, 1.2719e-12, 2.9517e-14],\n",
       "        [6.7955e-07, 4.8716e-11, 6.6370e-07, 4.6834e-09, 1.6498e-07, 4.6364e-15,\n",
       "         2.0293e-08, 2.1943e-11, 1.9681e-08, 1.8491e-05, 1.9241e-12, 6.3796e-05,\n",
       "         7.7252e-17, 7.6349e-12, 2.2090e-10, 1.1891e-15, 3.1408e-08, 6.1496e-15,\n",
       "         2.8789e-13, 2.1551e-02, 9.7835e-01, 3.8373e-08, 8.7702e-13, 9.0181e-11,\n",
       "         1.6308e-05, 4.3812e-12, 9.4476e-15],\n",
       "        [7.7934e-11, 2.0607e-09, 5.5180e-05, 3.5939e-09, 1.6912e-06, 7.7582e-08,\n",
       "         2.1119e-12, 3.5893e-11, 3.7127e-10, 7.7084e-09, 1.3469e-07, 2.7972e-07,\n",
       "         8.6259e-12, 1.8784e-03, 4.5493e-04, 6.2754e-01, 1.9231e-04, 4.1779e-10,\n",
       "         6.0385e-05, 2.2522e-07, 5.3897e-07, 2.0597e-05, 4.3501e-13, 1.2916e-07,\n",
       "         3.6979e-01, 2.9378e-08, 2.7892e-11],\n",
       "        [8.8065e-09, 1.4690e-04, 1.9862e-05, 3.8407e-08, 7.6152e-08, 7.1989e-12,\n",
       "         9.5434e-11, 2.1641e-07, 5.3638e-09, 2.0643e-08, 6.4877e-10, 1.4118e-02,\n",
       "         4.4283e-11, 2.8704e-05, 3.8206e-03, 1.4188e-10, 7.2322e-01, 2.4760e-08,\n",
       "         1.4033e-04, 4.8933e-08, 8.5709e-07, 1.9782e-06, 9.9065e-08, 9.2512e-10,\n",
       "         2.5850e-01, 3.4688e-07, 1.8002e-10],\n",
       "        [5.4459e-14, 1.2444e-08, 3.2858e-06, 9.8884e-06, 1.7350e-11, 1.9971e-09,\n",
       "         2.8320e-07, 1.2003e-12, 2.7561e-10, 6.1739e-12, 7.5947e-04, 5.9870e-06,\n",
       "         2.8088e-09, 5.1304e-07, 7.5357e-07, 2.0542e-08, 3.2429e-02, 6.0873e-12,\n",
       "         2.6353e-04, 7.4122e-06, 2.2237e-08, 9.4560e-01, 9.5726e-11, 1.0953e-14,\n",
       "         2.0924e-02, 7.3264e-09, 3.1278e-10],\n",
       "        [7.0010e-04, 2.0603e-13, 1.5530e-06, 1.0714e-01, 1.3761e-02, 1.2612e-04,\n",
       "         7.6711e-10, 3.4976e-10, 3.8315e-03, 3.6186e-10, 1.4581e-02, 3.2739e-09,\n",
       "         3.2324e-08, 2.5105e-04, 3.8956e-04, 5.9131e-01, 8.0743e-06, 1.8042e-07,\n",
       "         7.6603e-02, 4.1685e-12, 9.1059e-05, 1.8683e-01, 8.9454e-07, 3.2527e-08,\n",
       "         3.8128e-03, 5.6180e-04, 1.6463e-06],\n",
       "        [3.3596e-09, 7.8607e-14, 5.9110e-12, 3.6200e-14, 5.8699e-06, 5.7815e-13,\n",
       "         1.5543e-11, 1.5396e-06, 3.5234e-10, 4.9895e-09, 2.1321e-10, 1.3153e-10,\n",
       "         3.2735e-16, 1.7703e-06, 1.3779e-07, 1.1539e-01, 1.4661e-12, 2.4710e-15,\n",
       "         4.6268e-11, 1.0071e-07, 8.8460e-01, 3.0025e-13, 6.1888e-13, 3.4303e-06,\n",
       "         1.5371e-09, 1.2719e-12, 2.9517e-14],\n",
       "        [6.7955e-07, 4.8716e-11, 6.6370e-07, 4.6834e-09, 1.6498e-07, 4.6364e-15,\n",
       "         2.0293e-08, 2.1943e-11, 1.9681e-08, 1.8491e-05, 1.9241e-12, 6.3796e-05,\n",
       "         7.7252e-17, 7.6349e-12, 2.2090e-10, 1.1891e-15, 3.1408e-08, 6.1496e-15,\n",
       "         2.8789e-13, 2.1551e-02, 9.7835e-01, 3.8373e-08, 8.7702e-13, 9.0181e-11,\n",
       "         1.6308e-05, 4.3812e-12, 9.4476e-15],\n",
       "        [7.7934e-11, 2.0607e-09, 5.5180e-05, 3.5939e-09, 1.6912e-06, 7.7582e-08,\n",
       "         2.1119e-12, 3.5893e-11, 3.7127e-10, 7.7084e-09, 1.3469e-07, 2.7972e-07,\n",
       "         8.6259e-12, 1.8784e-03, 4.5493e-04, 6.2754e-01, 1.9231e-04, 4.1779e-10,\n",
       "         6.0385e-05, 2.2522e-07, 5.3897e-07, 2.0597e-05, 4.3501e-13, 1.2916e-07,\n",
       "         3.6979e-01, 2.9378e-08, 2.7892e-11],\n",
       "        [3.5553e-07, 3.5517e-13, 9.0477e-07, 4.5008e-07, 1.1903e-04, 2.6635e-07,\n",
       "         1.2269e-06, 1.8362e-11, 1.3075e-05, 2.1465e-10, 3.1316e-04, 2.1880e-07,\n",
       "         4.8382e-13, 5.0765e-07, 1.4076e-07, 4.5624e-06, 5.6441e-09, 3.9639e-11,\n",
       "         1.7859e-04, 1.1284e-06, 1.4613e-03, 9.9106e-01, 1.3543e-08, 1.6731e-10,\n",
       "         6.8428e-03, 9.6876e-07, 4.3516e-09],\n",
       "        [3.9029e-06, 6.0070e-12, 3.8441e-08, 2.6339e-07, 6.2160e-04, 6.2242e-09,\n",
       "         5.7254e-12, 4.4297e-16, 1.1003e-03, 4.8647e-13, 4.5140e-10, 6.6512e-10,\n",
       "         1.3452e-14, 7.0387e-07, 8.1896e-06, 2.6556e-03, 1.5284e-14, 2.6730e-10,\n",
       "         2.3882e-13, 1.0541e-09, 1.8694e-06, 9.9547e-01, 8.2190e-12, 1.3838e-04,\n",
       "         1.8301e-06, 5.1397e-07, 8.2864e-12],\n",
       "        [8.8065e-09, 1.4690e-04, 1.9862e-05, 3.8407e-08, 7.6152e-08, 7.1989e-12,\n",
       "         9.5434e-11, 2.1641e-07, 5.3638e-09, 2.0643e-08, 6.4877e-10, 1.4118e-02,\n",
       "         4.4283e-11, 2.8704e-05, 3.8206e-03, 1.4188e-10, 7.2322e-01, 2.4760e-08,\n",
       "         1.4033e-04, 4.8933e-08, 8.5709e-07, 1.9782e-06, 9.9065e-08, 9.2512e-10,\n",
       "         2.5850e-01, 3.4688e-07, 1.8002e-10],\n",
       "        [5.4459e-14, 1.2444e-08, 3.2858e-06, 9.8884e-06, 1.7350e-11, 1.9971e-09,\n",
       "         2.8320e-07, 1.2003e-12, 2.7561e-10, 6.1739e-12, 7.5947e-04, 5.9870e-06,\n",
       "         2.8088e-09, 5.1304e-07, 7.5357e-07, 2.0542e-08, 3.2429e-02, 6.0873e-12,\n",
       "         2.6353e-04, 7.4122e-06, 2.2237e-08, 9.4560e-01, 9.5726e-11, 1.0953e-14,\n",
       "         2.0924e-02, 7.3264e-09, 3.1278e-10],\n",
       "        [7.0010e-04, 2.0603e-13, 1.5530e-06, 1.0714e-01, 1.3761e-02, 1.2612e-04,\n",
       "         7.6711e-10, 3.4976e-10, 3.8315e-03, 3.6186e-10, 1.4581e-02, 3.2739e-09,\n",
       "         3.2324e-08, 2.5105e-04, 3.8956e-04, 5.9131e-01, 8.0743e-06, 1.8042e-07,\n",
       "         7.6603e-02, 4.1685e-12, 9.1059e-05, 1.8683e-01, 8.9454e-07, 3.2527e-08,\n",
       "         3.8128e-03, 5.6180e-04, 1.6463e-06],\n",
       "        [3.3596e-09, 7.8607e-14, 5.9110e-12, 3.6200e-14, 5.8699e-06, 5.7815e-13,\n",
       "         1.5543e-11, 1.5396e-06, 3.5234e-10, 4.9895e-09, 2.1321e-10, 1.3153e-10,\n",
       "         3.2735e-16, 1.7703e-06, 1.3779e-07, 1.1539e-01, 1.4661e-12, 2.4710e-15,\n",
       "         4.6268e-11, 1.0071e-07, 8.8460e-01, 3.0025e-13, 6.1888e-13, 3.4303e-06,\n",
       "         1.5371e-09, 1.2719e-12, 2.9517e-14],\n",
       "        [6.7955e-07, 4.8716e-11, 6.6370e-07, 4.6834e-09, 1.6498e-07, 4.6364e-15,\n",
       "         2.0293e-08, 2.1943e-11, 1.9681e-08, 1.8491e-05, 1.9241e-12, 6.3796e-05,\n",
       "         7.7252e-17, 7.6349e-12, 2.2090e-10, 1.1891e-15, 3.1408e-08, 6.1496e-15,\n",
       "         2.8789e-13, 2.1551e-02, 9.7835e-01, 3.8373e-08, 8.7702e-13, 9.0181e-11,\n",
       "         1.6308e-05, 4.3812e-12, 9.4476e-15],\n",
       "        [3.5346e-11, 6.8407e-11, 2.4117e-07, 5.8621e-06, 6.8740e-09, 4.5045e-05,\n",
       "         6.3502e-10, 7.1710e-14, 2.1430e-08, 3.6782e-12, 3.7406e-04, 6.6414e-10,\n",
       "         6.7119e-11, 2.2906e-07, 1.8729e-07, 9.9441e-01, 1.3617e-05, 2.4940e-11,\n",
       "         2.2001e-04, 2.8591e-09, 3.2084e-08, 1.2743e-03, 3.6685e-16, 3.2823e-10,\n",
       "         3.6587e-03, 2.7303e-07, 1.2413e-08],\n",
       "        [8.8065e-09, 1.4690e-04, 1.9862e-05, 3.8407e-08, 7.6152e-08, 7.1989e-12,\n",
       "         9.5434e-11, 2.1641e-07, 5.3638e-09, 2.0643e-08, 6.4877e-10, 1.4118e-02,\n",
       "         4.4283e-11, 2.8704e-05, 3.8206e-03, 1.4188e-10, 7.2322e-01, 2.4760e-08,\n",
       "         1.4033e-04, 4.8933e-08, 8.5709e-07, 1.9782e-06, 9.9065e-08, 9.2512e-10,\n",
       "         2.5850e-01, 3.4688e-07, 1.8002e-10],\n",
       "        [5.4459e-14, 1.2444e-08, 3.2858e-06, 9.8884e-06, 1.7350e-11, 1.9971e-09,\n",
       "         2.8320e-07, 1.2003e-12, 2.7561e-10, 6.1739e-12, 7.5947e-04, 5.9870e-06,\n",
       "         2.8088e-09, 5.1304e-07, 7.5357e-07, 2.0542e-08, 3.2429e-02, 6.0873e-12,\n",
       "         2.6353e-04, 7.4122e-06, 2.2237e-08, 9.4560e-01, 9.5726e-11, 1.0953e-14,\n",
       "         2.0924e-02, 7.3264e-09, 3.1278e-10],\n",
       "        [7.0010e-04, 2.0603e-13, 1.5530e-06, 1.0714e-01, 1.3761e-02, 1.2612e-04,\n",
       "         7.6711e-10, 3.4976e-10, 3.8315e-03, 3.6186e-10, 1.4581e-02, 3.2739e-09,\n",
       "         3.2324e-08, 2.5105e-04, 3.8956e-04, 5.9131e-01, 8.0743e-06, 1.8042e-07,\n",
       "         7.6603e-02, 4.1685e-12, 9.1059e-05, 1.8683e-01, 8.9454e-07, 3.2527e-08,\n",
       "         3.8128e-03, 5.6180e-04, 1.6463e-06],\n",
       "        [3.3596e-09, 7.8607e-14, 5.9110e-12, 3.6200e-14, 5.8699e-06, 5.7815e-13,\n",
       "         1.5543e-11, 1.5396e-06, 3.5234e-10, 4.9895e-09, 2.1321e-10, 1.3153e-10,\n",
       "         3.2735e-16, 1.7703e-06, 1.3779e-07, 1.1539e-01, 1.4661e-12, 2.4710e-15,\n",
       "         4.6268e-11, 1.0071e-07, 8.8460e-01, 3.0025e-13, 6.1888e-13, 3.4303e-06,\n",
       "         1.5371e-09, 1.2719e-12, 2.9517e-14],\n",
       "        [1.4992e-11, 8.8668e-15, 1.1253e-08, 3.7587e-06, 1.4176e-10, 4.6800e-09,\n",
       "         3.0656e-07, 4.8042e-15, 3.5021e-12, 8.5628e-11, 5.5613e-03, 6.3392e-09,\n",
       "         2.0105e-14, 9.2139e-15, 7.7339e-14, 1.5993e-10, 2.3311e-10, 4.4525e-18,\n",
       "         4.9791e-09, 9.9442e-01, 8.7687e-06, 2.8023e-06, 9.9319e-13, 2.0226e-12,\n",
       "         6.3934e-06, 1.4374e-13, 1.1909e-10],\n",
       "        [2.8948e-04, 1.5617e-15, 7.0974e-10, 3.9675e-08, 9.9882e-01, 1.0484e-12,\n",
       "         2.7338e-18, 8.4452e-14, 7.9997e-08, 1.8431e-11, 2.9662e-10, 6.9033e-12,\n",
       "         6.4719e-13, 6.1737e-08, 2.1943e-06, 8.8392e-04, 9.7606e-08, 1.4805e-11,\n",
       "         3.1535e-11, 1.2102e-12, 2.8365e-06, 4.6401e-10, 9.0769e-13, 1.4206e-06,\n",
       "         2.5415e-07, 7.7928e-09, 3.6545e-13],\n",
       "        [2.3923e-07, 1.0190e-10, 9.9027e-07, 7.4180e-11, 1.8473e-08, 2.0552e-07,\n",
       "         1.3253e-06, 2.1998e-11, 2.3797e-10, 1.1598e-07, 1.3956e-06, 1.3027e-08,\n",
       "         2.0031e-13, 1.6946e-09, 4.7202e-08, 1.5252e-02, 1.6069e-12, 1.3680e-13,\n",
       "         4.4234e-09, 3.4039e-01, 6.4344e-01, 6.5609e-09, 5.8967e-14, 9.1409e-04,\n",
       "         2.9181e-06, 2.0611e-13, 3.5693e-07],\n",
       "        [7.5137e-08, 4.8401e-14, 2.0594e-09, 3.1349e-06, 8.1341e-01, 1.0197e-04,\n",
       "         6.2200e-07, 2.9908e-04, 1.2111e-05, 3.3388e-09, 6.5834e-02, 1.4879e-08,\n",
       "         3.1575e-09, 4.5887e-07, 4.6090e-08, 7.2928e-02, 1.7780e-07, 8.1763e-09,\n",
       "         4.5628e-02, 9.7746e-08, 2.7348e-05, 1.1969e-03, 2.7050e-04, 5.9407e-08,\n",
       "         2.7580e-04, 1.0031e-05, 3.8360e-07],\n",
       "        [2.0800e-07, 2.1483e-12, 1.1816e-09, 2.8593e-02, 5.7073e-02, 1.1569e-07,\n",
       "         6.7287e-07, 2.3636e-06, 4.1365e-08, 3.6001e-05, 2.6819e-01, 2.4716e-08,\n",
       "         1.0216e-06, 3.7610e-07, 3.5457e-08, 4.9499e-04, 2.2546e-04, 3.2798e-12,\n",
       "         4.3356e-07, 1.1416e-02, 6.3392e-01, 1.8754e-11, 3.4971e-07, 7.9402e-10,\n",
       "         5.1386e-05, 5.7108e-09, 1.7615e-09],\n",
       "        [3.9099e-07, 1.7540e-13, 1.0853e-07, 9.6704e-01, 1.8996e-03, 6.6040e-09,\n",
       "         4.6868e-10, 4.1177e-14, 1.3911e-08, 4.9791e-09, 4.4224e-05, 2.5173e-11,\n",
       "         8.5434e-10, 1.9656e-09, 1.1717e-11, 8.0812e-05, 1.6768e-06, 2.8203e-13,\n",
       "         4.3848e-11, 6.7763e-04, 3.0255e-02, 4.7082e-08, 2.3702e-11, 1.7080e-09,\n",
       "         4.2039e-08, 1.3540e-08, 2.5688e-09]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum() # normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  1, 14,  0,  1,  1,  2,  9,  4,  0,  1,  1,  2,  9,  4,  1,\n",
       "         8,  0,  1,  1,  2,  9, 18,  0,  1,  1,  2, 18,  9,  5, 12, 12,  1,  0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4690e-04, 1.2444e-08, 1.5530e-06, 7.8607e-14, 2.6853e-13, 1.3313e-03,\n",
       "        1.4690e-04, 1.2444e-08, 1.5530e-06, 4.9895e-09, 1.6498e-07, 7.7934e-11,\n",
       "        1.4690e-04, 1.2444e-08, 1.5530e-06, 4.9895e-09, 1.6498e-07, 2.0607e-09,\n",
       "        1.3075e-05, 3.9029e-06, 1.4690e-04, 1.2444e-08, 1.5530e-06, 4.9895e-09,\n",
       "        2.8789e-13, 3.5346e-11, 1.4690e-04, 1.2444e-08, 1.5530e-06, 4.6268e-11,\n",
       "        8.5628e-11, 1.0484e-12, 2.0031e-13, 3.1575e-09, 2.1483e-12, 3.9099e-07])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(36), Y] # probabilities of the true characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.8125)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(36), Y].log().mean() # cross entropy loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
